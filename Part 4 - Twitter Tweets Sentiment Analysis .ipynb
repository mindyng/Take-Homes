{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mindyng/anaconda/lib/python2.7/site-packages/numpy/lib/arraysetops.py:395: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww that bummer you shoulda got david carr of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can not update his facebook b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dived many times for the ball managed to save ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no it not behaving at all mad why am here beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  awww that bummer you shoulda got david carr of...       0\n",
       "1  is upset that he can not update his facebook b...       0\n",
       "2  dived many times for the ball managed to save ...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  no it not behaving at all mad why am here beca...       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1596019 entries, 0 to 1596018\n",
      "Data columns (total 2 columns):\n",
      "text      1596019 non-null object\n",
      "target    1596019 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Dev / Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train any model, we first consider how to split the data. Here I chose to split the data into three chunks: train, development, test. I referenced Andrew Ng's \"deeplearning.ai\" course on how to split the data.\n",
    "\n",
    "- Train set: The sample of data used for learning\n",
    "- Development set (Hold-out cross-validation set): The sample of data used to tune the parameters of a classifier, and provide an unbiased evaluation of a model.\n",
    "- Test set: The sample of data used only to assess the performance of a final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio I decided to split my data is 98/1/1, 98% of data as the training set, and 1% for the dev set, and the final 1% for the test set. The rationale behind this ratio comes from the size of my whole data set. The dataset has more than 1.5 million entries. In this case, only 1% of the whole data gives me more than 15,000 entries. This is more than enough to evaluate the model and refine the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is splitting the data into only train and test set, and run k-fold cross-validation on the training set, so that you can have an unbiased evaluation of a model. But considering the size of the data, I have decided to use the train set only to train a model, and evaluate on the dev set, so that I can quickly test different algorithms and run this process iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = my_df.text\n",
    "y = my_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mindyng/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1564098 entries with 50.00% negative, 50.00% positive\n",
      "Validation set has total 15960 entries with 50.40% negative, 49.60% positive\n",
      "Test set has total 15961 entries with 50.26% negative, 49.74% positive\n"
     ]
    }
   ],
   "source": [
    "print \"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100)\n",
    "print \"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100)\n",
    "print \"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing various machine learning algorithms, baseline provides a point of reference to compare.\n",
    "The most popular baseline is the Zero Rule (ZeroR). ZeroR classifier simply predicts the majority category (class). Although there is no predictability power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods. As you can see from the above validation set class division, the majority class is negative with 50.40%, which means if a classifier predicts negative for every validation data, it will get 50.40% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another baseline I wanted to compare the validation results with is TextBlob. Textblob is a python library for processing textual data. Apart from other useful tools such as POS tagging, n-gram, The package has built-in sentiment classification. This is a so-called out-of-the-box sentiment analysis tool, and in addition to the null accuracy, I will also keep in mind of the accuracy I get from TextBlob sentiment analysis to see how my model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 116 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tbresult = [TextBlob(i).sentiment.polarity for i in x_validation]\n",
    "tbpred = [0 if n < 0 else 1 for n in tbresult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 60.65%\n",
      "--------------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "          predicted_positive  predicted_negative\n",
      "positive                7094                 822\n",
      "negative                5458                2586\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.32      0.45      8044\n",
      "          1       0.57      0.90      0.69      7916\n",
      "\n",
      "avg / total       0.66      0.61      0.57     15960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conmat = np.array(confusion_matrix(y_validation, tbpred, labels=[1,0]))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['positive', 'negative'],\n",
    "                         columns=['predicted_positive','predicted_negative'])\n",
    "print \"Accuracy Score: {0:.2f}%\".format(accuracy_score(y_validation, tbpred)*100)\n",
    "print \"-\"*80\n",
    "print \"Confusion Matrix\\n\"\n",
    "print confusion\n",
    "print \"-\"*80\n",
    "print \"Classification Report\\n\"\n",
    "print classification_report(y_validation, tbpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob sentiment analysis yielded 60.65% accuracy on the validation set, which is 10.25% more accurate than null accuracy (50.40%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use text in machine learning algorithms, weâ€™ll have to convert them to a numerical representation.\n",
    "One of the methods is called bag-of-words approach. The bag of words model ignores grammar and order of words.\n",
    "Once we have a corpus (text data) then first, a list of vocabulary is created based on the entire corpus.\n",
    "Then each document or data entry is represented as numerical vectors based on the vocabulary built from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With count vectorizer, we merely count the appearance of the words in each text. For example, let's say we have 3 documents in a corpus: \"I love dogs\", \"I hate dogs and knitting\", \"Knitting is my hobby and my passion\".\n",
    "If we build vocabulary from these three sentences and represent each document as count vectors, it will look like below pictures.\n",
    "\n",
    "![title](img/cvec.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if the size of the corpus gets big, the number of vocabulary gets too big to process. With my 1.5 million tweets, if I build vocabulary without limiting the number of vocabulary, I will have more than 260,000 vocabularies. This means that the shape of training data will be around 1,500,000 x 260,000, this sounds too big to train various different models with. So I decided to limit the number of vocabularies, but I also wanted to see how the performance varies depending on the number of vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing I wanted to explore is stopwords. Stop Words are words which do not contain important significance, such as \"the\", \"of\", etc. It is often assumed that removing stopwords is a necessary step, and will improve the model performance. But I wanted to see for myself if this is really the case. So I ran the same test with and without stop words and compared the result. In addition, I also defined my custom stopwords list, which contains top 10 most frequent words in the corpus: \"to\", \"the\", \"my\", \"it\", \"and\", \"you\", \"not\", \"is\", \"in\", \"for\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model I chose to evaluate different count vectors is the logistic regression. It is one of the linear models, so computationally scalable to big data, compared to models like KNN or random forest. And once I have the optimal number of features and make a decision on whether to remove stop words or not, then I will try different models with the chosen number of vocabularies' count vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I define two functions to iteratively train on a different number of features, then check the accuracy of logistic regression on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print \"null accuracy: {0:.2f}%\".format(null_accuracy*100)\n",
    "    print \"accuracy score: {0:.2f}%\".format(accuracy*100)\n",
    "    if accuracy > null_accuracy:\n",
    "        print \"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100)\n",
    "    elif accuracy == null_accuracy:\n",
    "        print \"model has the same accuracy with the null accuracy\"\n",
    "    else:\n",
    "        print \"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100)\n",
    "    print \"train and test time: {0:.2f}s\".format(train_test_time)\n",
    "    print \"-\"*80\n",
    "    return accuracy, train_test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(10000,100001,10000)\n",
    "\n",
    "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print \"\\n\"\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print \"Validation result for {} features\".format(n)\n",
    "        nfeature_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy,tt_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT FOR UNIGRAM WITHOUT STOP WORDS\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Validation result for 10000 features\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 77.36%\n",
      "model is 26.95% more accurate than null accuracy\n",
      "train and test time: 66.54s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 20000 features\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 77.39%\n",
      "model is 26.99% more accurate than null accuracy\n",
      "train and test time: 73.05s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 30000 features\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 77.53%\n",
      "model is 27.12% more accurate than null accuracy\n",
      "train and test time: 85.54s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 40000 features\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 77.53%\n",
      "model is 27.12% more accurate than null accuracy\n",
      "train and test time: 90.13s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 50000 features\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 77.46%\n",
      "model is 27.06% more accurate than null accuracy\n",
      "train and test time: 102.43s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 60000 features\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 77.54%\n",
      "model is 27.14% more accurate than null accuracy\n",
      "train and test time: 95.35s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 70000 features\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 77.55%\n",
      "model is 27.15% more accurate than null accuracy\n",
      "train and test time: 114.15s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 80000 features\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 77.57%\n",
      "model is 27.17% more accurate than null accuracy\n",
      "train and test time: 115.59s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 90000 features\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\"\n",
    "feature_result_wosw = nfeature_accuracy_checker(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = 'term_freq_df.csv'\n",
    "term_freq_df = pd.read_csv(csv,index_col=0)\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "a = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))\n",
    "b = text.ENGLISH_STOP_WORDS\n",
    "set(a).issubset(set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_stop_words = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print \"RESULT FOR UNIGRAM WITHOUT CUSTOM STOP WORDS (Top 10 frequent words)\\n\"\n",
    "feature_result_wocsw = nfeature_accuracy_checker(stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ug_wocsw = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ug_wosw = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='with stop words')\n",
    "plt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy,label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy,label='without stop words')\n",
    "plt.title(\"Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the evaluation result, removing stop words did not improve the model performance, but keeping the stop words yielded better performance. I wouldn't say that removing stopwords are not helping the model performance every time, but as empirical findings, in this particular setting, keeping the stop words improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Wikipedia, \"n-gram is a contiguous sequence of n items from a given sequence of text or speech\". In other words, n-grams are simply all combinations of adjacent words or letters of length n that you can find in your source text. Below picture represents well how n-grams are constructed out of source text.\n",
    "\n",
    "![title](img/bigram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will extend the bag-of-words to trigrams, and see if it affects the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print \"RESULT FOR BIGRAM WITH STOP WORDS\\n\"\n",
    "feature_result_bg = nfeature_accuracy_checker(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print \"RESULT FOR TRIGRAM WITH STOP WORDS\\n\"\n",
    "feature_result_tg = nfeature_accuracy_checker(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise the results we got from unigram, bigram, and trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfeatures_plot_tg = pd.DataFrame(feature_result_tg,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_bg = pd.DataFrame(feature_result_bg,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram')\n",
    "plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram')\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram')\n",
    "plt.title(\"N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation set accuracy for each n-gram is as below.\n",
    "- unigram: 80,000 & 90,000 features at validation accuracy 80.28%\n",
    "- bigram: 70,000 features at validation accuracy 82.25%\n",
    "- trigram: 80,000 features at validation accuracy 82.44%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I defined another function to take a closer look at best performing number of features with each n-gram.\n",
    "Below function not only reports accuracy but also gives confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_and_evaluate(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "    confusion = pd.DataFrame(conmat, index=['negative', 'positive'],\n",
    "                         columns=['predicted_negative','predicted_positive'])\n",
    "    print \"null accuracy: {0:.2f}%\".format(null_accuracy*100)\n",
    "    print \"accuracy score: {0:.2f}%\".format(accuracy*100)\n",
    "    if accuracy > null_accuracy:\n",
    "        print \"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100)\n",
    "    elif accuracy == null_accuracy:\n",
    "        print \"model has the same accuracy with the null accuracy\"\n",
    "    else:\n",
    "        print \"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100)\n",
    "    print \"-\"*80\n",
    "    print \"Confusion Matrix\\n\"\n",
    "    print confusion\n",
    "    print \"-\"*80\n",
    "    print \"Classification Report\\n\"\n",
    "    print classification_report(y_test, y_pred, target_names=['negative','positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I run the defined function, let me briefly explain about confusion matrix and classification report.\n",
    "In order to evaluate the performance of a model, there are many different metrics that can be used.\n",
    "Below I will talk in case of binary classification, in which the target variable only has two classes to be predicted.\n",
    "In the case of this project, the classes are either \"negative\" or \"positive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One obvious measure of performance can be accuracy. It is the number of times the model predicted correctly for the class over the number of the whole data set. But in case of classification, this can be broken down further.\n",
    "Below is a representation of confusion matrix.\n",
    "\n",
    "![title](img/cm01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above matrix, each row represents the instances in an actual class while each column represents the instances in a predicted class, and it can be also presented swapping rows and columns (column for the actual class, row for predicted class). So the accuracy (ACC) I mentioned above can be expressed as below.\n",
    "\n",
    "$${ACC} = \\frac {True Positive + True Negative}{Positive + Negative} = \\frac {True Positive + True Negative}{True Positive + False Positive + True Negative + False Negative}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the distribution of the classes in data is well balanced, accuracy can give you a good picture of how the model is performing. But when you have skewed data, for example, one of the class is dominant in your data set, then accuracy might not be enough to evaluate your model. Let's say you have a dataset which contains 80% positive class, and 20% negative class. This means that by predicting every data into the positive class, the model will get 80% accuracy. In this case, you might want to explore further into the confusion matrix and try different evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be 9 different metrics, just from the combination of numbers from confusion matrix, but I will talk about two of them in particular, and another metric which combines these two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Precision\" (also called Positive Predictive Value) tells you what proportion of data predicted as positive actually is positive. In other words, the proportion of True Positive in the set of all positive predicted data.\n",
    "$${PPV(Precision)} = \\frac {True Positive}{True Positive + False Positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Recall\" (also called Sensitivity, Hit Rate, True Positive Rate) tells you what proportion of data that actually is positive were predicted positive. In other words, the proportion of True Positive in the set of all actual positive data.\n",
    "\n",
    "$${TPR(Recall)} = \\frac {True Positive}{Positive} = \\frac {True Positive}{True Positive + False Negative}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the image of confusion matrix of cancer diagnose. If you think of \"cancer\" as positive class, \"no cancer\" as a negative class, the image explains well how to think of precision and recall in terms of the confusion matrix.\n",
    "![title](img/cm02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the F1 score is the harmonic mean of precision and recall. The harmonic mean is a specific type of average, which is used when dealing with averages of units, like rates and ratios. So by calculating the harmonic mean of the two metrics, it will give you a good idea of how the model is performing both in terms of precision and recall. The formula is as below\n",
    "$${F1} = 2\\cdot\\frac {Precision\\cdot Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ug_cvec = CountVectorizer(max_features=80000)\n",
    "ug_pipeline = Pipeline([\n",
    "        ('vectorizer', ug_cvec),\n",
    "        ('classifier', lr)\n",
    "    ])\n",
    "train_test_and_evaluate(ug_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bg_cvec = CountVectorizer(max_features=70000,ngram_range=(1, 2))\n",
    "bg_pipeline = Pipeline([\n",
    "        ('vectorizer', bg_cvec),\n",
    "        ('classifier', lr)\n",
    "    ])\n",
    "train_test_and_evaluate(bg_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tg_cvec = CountVectorizer(max_features=80000,ngram_range=(1, 3))\n",
    "tg_pipeline = Pipeline([\n",
    "        ('vectorizer', tg_cvec),\n",
    "        ('classifier', lr)\n",
    "    ])\n",
    "train_test_and_evaluate(tg_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above classification reports, we can see that model has slightly higher precision in negative class and higher recall in positive class. But this averages out by calculating the F1 score, and for both classes, we get the almost same F1 score for both positive and negative class. There is also a way to visualise the model performance by plotting ROC curve, but I will explain more in detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
